This project demonstrates the practical application of Apache Spark for processing streaming data by performing a word count on a text file using the Spark Shell and Scala. The objective of the assignment was to gain hands-on experience with big data tools, including installing the JDK and Spark, loading a text dataset, and analyzing it efficiently to extract meaningful statistics. By working with a large text file, the project highlights how Spark handles data in a high-performance manner, providing insight into the frequency of words and symbols, and enabling analysis of data streams in real-time or batch processes.

The analysis results showed that the word “Hadoop” appeared six times, while the most common word, “the,” occurred 23 times, and the most frequent symbol was the comma, appearing 41 times. Nearly 200 words in the file were counted only once, demonstrating the variety of word usage. This lab provided insight into Spark’s capabilities for processing large datasets, caching files for optimized performance, and tracking job execution through the web console. Overall, this project emphasizes the importance of streaming data techniques for real-world analytics, showcasing Spark as a robust framework for high-performance big data processing.
